{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Matrix","text":"<p>LLM Matrix is a tool for running, evaluating, and comparing different language models across a matrix of hyperparameters.</p>"},{"location":"#overview","title":"Overview","text":"<p>LLM Matrix enables you to:</p> <ul> <li>Define test suites with multiple cases in YAML</li> <li>Run test cases across different models and parameters</li> <li>Evaluate model responses using specialized metrics</li> <li>Generate comprehensive reports to analyze performance</li> <li>Compare results across models and parameters</li> </ul>"},{"location":"#key-concepts","title":"Key Concepts","text":""},{"location":"#test-suites","title":"Test Suites","text":"<p>A test suite is a YAML file that defines:</p> <ul> <li>Test cases (inputs and expected outputs)</li> <li>Templates for system and user prompts</li> <li>Models to test</li> <li>Hyperparameters to vary</li> <li>Metrics for evaluation</li> </ul>"},{"location":"#matrix-execution","title":"Matrix Execution","text":"<p>LLM Matrix runs each test case against a matrix of parameters, such as:</p> <ul> <li>Different LLM models (e.g., GPT-4, Claude)</li> <li>Various temperature settings</li> <li>Different prompt templates</li> </ul>"},{"location":"#metrics","title":"Metrics","text":"<p>Custom metrics evaluate model outputs by comparing them to ideal answers:</p> <ul> <li>QA metrics for question-answering tasks</li> <li>Binary classification metrics (YES/NO questions)</li> <li>List matching metrics for enumeration tasks</li> </ul>"},{"location":"#result-analysis","title":"Result Analysis","text":"<p>After running a test suite, you can analyze:</p> <ul> <li>Model performance by score</li> <li>Statistical summaries</li> <li>Per-case breakdowns</li> <li>Comparisons across models and parameters</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install llm-matrix\n</code></pre> <p>For optional features:</p> <pre><code># Excel report support\npip install \"llm-matrix[excel]\"\n\n# MLflow integration\npip install \"llm-matrix[mlflow]\"\n\n# LinkML mapping support\npip install \"llm-matrix[map]\"\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Tutorial - Get started with LLM Matrix</li> <li>Configuration - Learn how to configure test suites</li> <li>Metrics - Available metrics for evaluation</li> <li>CLI - Command-line interface reference</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>LLM Matrix provides a command-line interface for running evaluations and managing results.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<pre><code>llm-matrix run my-suite.yaml\n</code></pre>"},{"location":"cli/#global-options","title":"Global Options","text":"<pre><code>--verbose, -v      Increase verbosity (can be used multiple times)\n--help             Show help message and exit\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#run","title":"<code>run</code>","text":"<p>Run a test suite against specified models.</p> <pre><code>llm-matrix run &lt;suite-path&gt; [options]\n</code></pre>"},{"location":"cli/#arguments","title":"Arguments","text":"<ul> <li><code>suite-path</code>: Path to the evaluation suite YAML file</li> </ul>"},{"location":"cli/#options","title":"Options","text":"<ul> <li><code>--store-path, -s &lt;path&gt;</code>: Path to the cache store (defaults to same directory as suite with .db extension)</li> <li><code>--runner-config, -C &lt;path&gt;</code>: Path to the runner config file</li> <li><code>--output-file, -o &lt;path&gt;</code>: Path to save output file</li> <li><code>--output-dir, -D &lt;path&gt;</code>: Directory to save output files (defaults to suite-name-output)</li> <li><code>--output-format, -F &lt;format&gt;</code>: Output format (csv, tsv, excel, jsonl, json, yaml)</li> </ul>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Basic usage\nllm-matrix run my-suite.yaml\n\n# Specify custom output location\nllm-matrix run my-suite.yaml -D ./results\n\n# Use custom runner configuration\nllm-matrix run my-suite.yaml -C runner-config.yaml\n\n# Increase verbosity for debugging\nllm-matrix run my-suite.yaml -vv\n\n# Output as Excel file\nllm-matrix run my-suite.yaml -o results.xlsx -F excel\n</code></pre>"},{"location":"cli/#convert","title":"<code>convert</code>","text":"<p>Convert between different file formats.</p> <pre><code>llm-matrix convert &lt;input-files&gt; [options]\n</code></pre>"},{"location":"cli/#arguments_1","title":"Arguments","text":"<ul> <li><code>input-files</code>: Paths to files to be converted</li> </ul>"},{"location":"cli/#options_1","title":"Options","text":"<ul> <li><code>--source, -s &lt;field&gt;</code>: Source field</li> <li><code>--target, -t &lt;field&gt;</code>: Target field</li> </ul>"},{"location":"cli/#output-formats","title":"Output Formats","text":"<p>LLM Matrix supports the following output formats:</p> <ul> <li><code>csv</code>: Comma-separated values</li> <li><code>tsv</code>: Tab-separated values</li> <li><code>excel</code>: Microsoft Excel format (requires the <code>excel</code> extra)</li> <li><code>jsonl</code>: JSON Lines format (one JSON object per line)</li> <li><code>json</code>: Single JSON array with all results</li> <li><code>yaml</code>: YAML format</li> </ul>"},{"location":"cli/#output-directory-structure","title":"Output Directory Structure","text":"<p>When using the default output directory, LLM Matrix creates:</p> <pre><code>suite-name-output/\n\u251c\u2500\u2500 by_model.csv             # Performance summary by model\n\u251c\u2500\u2500 by_model_ideal.csv       # Performance by model and ideal answer\n\u251c\u2500\u2500 grouped_by_input.tsv     # Detailed view of each test case\n\u251c\u2500\u2500 grouped_by_input.xlsx    # Excel version of the above\n\u251c\u2500\u2500 results.csv              # Raw results\n\u251c\u2500\u2500 results.html             # HTML view of raw results\n\u251c\u2500\u2500 summary.csv              # Statistical summary\n\u2514\u2500\u2500 summary.html             # HTML view of summary\n</code></pre>"},{"location":"contributing/","title":"Contributing to LLM Matrix","text":"<p>We welcome contributions to LLM Matrix! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#development-environment","title":"Development Environment","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Poetry for dependency management</li> </ul>"},{"location":"contributing/#setup","title":"Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/monarch-initiative/llm-matrix.git\ncd llm-matrix\n</code></pre> <ol> <li>Install dependencies with Poetry:</li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>poetry shell\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools to maintain code quality:</p> <ul> <li>Black: Code formatter with 120 character line length</li> <li>Ruff: Linter with various rules (flake8, isort, etc.)</li> <li>MyPy: Static type checking</li> </ul> <p>To check code quality:</p> <pre><code># Run type checking\nmake mypy\n\n# Run linters\ntox -e lint\n\n# Auto-fix linting issues where possible\ntox -e lint-fix\n\n# Check spelling\ntox -e codespell\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests, doctest, mypy, and codespell\nmake test\n\n# Run only pytest tests\nmake pytest\n\n# Run a specific test\npoetry run pytest tests/test_file.py::TestClass::test_function\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Create a new branch for your changes:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make your changes following our code style guidelines.</p> </li> <li> <p>Add tests for new functionality.</p> </li> <li> <p>Ensure all tests pass:</p> </li> </ol> <pre><code>make test\n</code></pre> <ol> <li>Update documentation as needed.</li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Push your changes to your fork:</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li> <p>Create a pull request on GitHub.</p> </li> <li> <p>Ensure the PR description clearly describes the problem and solution.</p> </li> </ol>"},{"location":"contributing/#adding-new-metrics","title":"Adding New Metrics","text":"<p>To add a new evaluation metric:</p> <ol> <li>Create a new class in <code>src/llm_matrix/metrics.py</code> that inherits from <code>Metric</code>:</li> </ol> <pre><code>class MyNewMetric(Metric):\n    def evaluate(self, case: TestCase, response: LLMResponse) -&gt; EvalResult:\n        # Implement your evaluation logic\n        score = calculate_score(case.ideal, response.text)\n\n        return EvalResult(\n            score=score,\n            explanation=\"Reason for this score\"\n        )\n</code></pre> <ol> <li>Register your metric:</li> </ol> <pre><code>register_metric(\"my_new_metric\", MyNewMetric())\n</code></pre> <ol> <li> <p>Add tests for your metric in <code>tests/test_metrics.py</code>.</p> </li> <li> <p>Update documentation in <code>docs/metrics/index.md</code>.</p> </li> </ol>"},{"location":"contributing/#creating-plugins","title":"Creating Plugins","text":"<p>To create a new plugin:</p> <ol> <li> <p>Create a new file in <code>src/llm_matrix/plugins/</code> (e.g., <code>my_plugin.py</code>).</p> </li> <li> <p>Implement the plugin interface:</p> </li> </ol> <pre><code>from llm_matrix.plugins.plugin import Plugin\n\nclass MyPlugin(Plugin):\n    def name(self) -&gt; str:\n        return \"my_plugin\"\n\n    # Implement required methods\n</code></pre> <ol> <li> <p>Register your plugin in <code>src/llm_matrix/plugins/__init__.py</code>.</p> </li> <li> <p>Add tests for your plugin in <code>tests/test_plugins/</code>.</p> </li> <li> <p>Update documentation as needed.</p> </li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs with the Material theme for documentation:</p> <ol> <li> <p>Update documentation as needed in the <code>docs/</code> directory.</p> </li> <li> <p>Preview documentation locally:</p> </li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>Ensure all links and references are correct.</li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<ol> <li>Update version number in <code>pyproject.toml</code>.</li> <li>Update CHANGELOG.md with changes in the release.</li> <li>Create a tag with the new version number.</li> <li>A GitHub Action will automatically build and publish to PyPI.</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have any questions, please open an issue on GitHub or reach out to the maintainers.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation of the LLM Matrix Python API.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":""},{"location":"api/#llmrunner","title":"LLMRunner","text":"<p>The main class for running evaluations.</p> <pre><code>from llm_matrix import LLMRunner\n\nrunner = LLMRunner(store_path=\"results.db\")\nresults = runner.run(suite)\n</code></pre> Source code in <code>src/llm_matrix/runner.py</code> <pre><code>@dataclass\nclass LLMRunner:\n\n    store_path: Optional[Path] = None\n    _aimodels: Optional[Dict[tuple, AIModel]] = None\n    _store: Optional[Store] = None\n    config: Optional[LLMRunnerConfig] = None\n\n    def run(self, suite: Suite) -&gt; List[TestCaseResult]:\n        \"\"\"\n        Run the suite of cases\n\n        :param suite:\n        :return:\n        \"\"\"\n        return list(self.run_iter(suite))\n\n    def run_iter(self, suite: Suite) -&gt; Iterator[TestCaseResult]:\n        \"\"\"\n        Run the suite of cases iterating over the results.\n\n        :param suite:\n        :return:\n        \"\"\"\n        logger.info(f\"Running suite {suite.name}\")\n        for params in iter_hyperparameters(suite.matrix):\n            logger.info(f\"Running {len(suite.cases)} with {params}\")\n            for n, case in enumerate(suite.cases):\n                logger.debug(f\"Running case {n+1}/{len(suite.cases)}\")\n                result = self.run_case(case, params, suite)\n                yield result\n\n    def run_case(self, case: TestCase, params: Dict[str, Any], suite: Suite) -&gt; TestCaseResult:\n        logger.info(f\"Running case {case.input} with {params}\")\n        store = self._get_store()\n        cached = store.get_result(suite, case, params)\n        if cached:\n            return cached\n        actual_params = copy(params)\n        if self.config and \"model\" in params:\n            model_logical_name = params[\"model\"]\n            actual_params[\"model\"] = self.config.model_name_map.get(model_logical_name, model_logical_name)\n            logger.info(f\"Mapping model {model_logical_name} to {actual_params['model']}\")\n        model = self.get_aimodel(actual_params, suite=suite)\n        template = self.get_template(case, suite)\n        response = model.prompt(case.input, template=template, case=case)\n        result = TestCaseResult(\n            case=case,\n            response=response,\n            hyperparameters=params,\n            metrics=template.metrics if template else None,\n        )\n        from llm_matrix.metrics import evaluate_result\n        evaluate_result(result, runner=self)\n        store.add_result(suite, result)\n        return result\n\n    def get_template(self, case, suite: Suite) -&gt; Optional[Template]:\n        tn = case.template\n        if not tn:\n            tn = suite.template\n        if not tn:\n            return None\n        return suite.templates[tn]\n\n    def get_aimodel(self, params: Dict[str, Any], suite: Suite=None) -&gt; AIModel:\n        if not self._aimodels:\n            self._aimodels = {}\n        key = tuple(sorted(params.items()))\n        if key not in self._aimodels:\n            if suite and suite.models:\n                bespoke_models = suite.models\n                model_name = params.get(\"model\")\n                if model_name and model_name in bespoke_models:\n                    model_info = bespoke_models[model_name]\n                    params = {**params, **model_info.parameters}\n                    plugins = model_info.plugins or []\n                    # TODO: proper mechanism for plugins\n                    if \"citeseek\" in plugins:\n                        from llm_matrix.plugins.citeseek_plugin import CiteseekPlugin\n                        self._aimodels[key] = CiteseekPlugin(parameters=params)\n            if key not in self._aimodels:\n                self._aimodels[key] = AIModel(parameters=params)\n        m = self._aimodels[key]\n        return self._aimodels[key]\n\n    def _get_store(self) -&gt; Store:\n        if not self._store:\n            if not self.store_path:\n                self.store_path = Path(\"results\") / \"cache.db\"\n            self.store_path.parent.mkdir(parents=True, exist_ok=True)\n            self._store = Store(self.store_path)\n        return self._store\n</code></pre>"},{"location":"api/#llm_matrix.LLMRunner.run","title":"<code>run(suite)</code>","text":"<p>Run the suite of cases</p> <p>Parameters:</p> Name Type Description Default <code>suite</code> <code>Suite</code> required <p>Returns:</p> Type Description <code>List[TestCaseResult]</code> Source code in <code>src/llm_matrix/runner.py</code> <pre><code>def run(self, suite: Suite) -&gt; List[TestCaseResult]:\n    \"\"\"\n    Run the suite of cases\n\n    :param suite:\n    :return:\n    \"\"\"\n    return list(self.run_iter(suite))\n</code></pre>"},{"location":"api/#llm_matrix.LLMRunner.run_iter","title":"<code>run_iter(suite)</code>","text":"<p>Run the suite of cases iterating over the results.</p> <p>Parameters:</p> Name Type Description Default <code>suite</code> <code>Suite</code> required <p>Returns:</p> Type Description <code>Iterator[TestCaseResult]</code> Source code in <code>src/llm_matrix/runner.py</code> <pre><code>def run_iter(self, suite: Suite) -&gt; Iterator[TestCaseResult]:\n    \"\"\"\n    Run the suite of cases iterating over the results.\n\n    :param suite:\n    :return:\n    \"\"\"\n    logger.info(f\"Running suite {suite.name}\")\n    for params in iter_hyperparameters(suite.matrix):\n        logger.info(f\"Running {len(suite.cases)} with {params}\")\n        for n, case in enumerate(suite.cases):\n            logger.debug(f\"Running case {n+1}/{len(suite.cases)}\")\n            result = self.run_case(case, params, suite)\n            yield result\n</code></pre>"},{"location":"api/#schema","title":"Schema","text":"<p>Data models for test cases, templates, responses, and results.</p>"},{"location":"api/#llm_matrix.schema.StrictBaseModel","title":"<code>StrictBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for Pydantic models that forbids extra fields.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class StrictBaseModel(BaseModel):\n    \"\"\"\n    Base class for Pydantic models that forbids extra fields.\n    \"\"\"\n    model_config = ConfigDict(extra=\"forbid\")\n\n    def as_flat_dict(self, simple=True, prefix = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert the model to a flat dictionary.\n\n        Suitable for conversion to a DataFrame.\n\n        :param simple:\n        :param prefix:\n        :return:\n        \"\"\"\n        def is_complex(v):\n            if isinstance(v, dict):\n                return True\n            if isinstance(v, list) and any(isinstance(i, dict) for i in v):\n                return True\n            return False\n        def mk_key(k):\n            if prefix:\n                return f\"{prefix}_{k}\"\n            return k\n        d = {mk_key(k): v for k, v in self.model_dump().items() if not is_complex(v)}\n        return d\n</code></pre>"},{"location":"api/#llm_matrix.schema.StrictBaseModel.as_flat_dict","title":"<code>as_flat_dict(simple=True, prefix=None)</code>","text":"<p>Convert the model to a flat dictionary.</p> <p>Suitable for conversion to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>simple</code> <code>True</code> <code>prefix</code> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>def as_flat_dict(self, simple=True, prefix = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert the model to a flat dictionary.\n\n    Suitable for conversion to a DataFrame.\n\n    :param simple:\n    :param prefix:\n    :return:\n    \"\"\"\n    def is_complex(v):\n        if isinstance(v, dict):\n            return True\n        if isinstance(v, list) and any(isinstance(i, dict) for i in v):\n            return True\n        return False\n    def mk_key(k):\n        if prefix:\n            return f\"{prefix}_{k}\"\n        return k\n    d = {mk_key(k): v for k, v in self.model_dump().items() if not is_complex(v)}\n    return d\n</code></pre>"},{"location":"api/#llm_matrix.schema.MetricEnum","title":"<code>MetricEnum</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for metrics to be evaluated.</p> <p>Designed to be extensible.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class MetricEnum(Enum):\n    \"\"\"\n    Enum for metrics to be evaluated.\n\n    Designed to be extensible.\n    \"\"\"\n    QA_WITH_EXPLANATION = \"qa_with_explanation\"\n    SIMPLE_QUESTION = \"simple_question\"\n    LIST_MEMBERSHIP = \"list_membership\"\n    RANKED_LIST = \"ranked_list\"\n    NARRATIVE_SIMILARITY = \"narrative_similarity\"\n    REVIEW = \"review\"\n    BLEU = \"bleu\"\n    ROUGE = \"rouge\"\n    METEOR = \"meteor\"\n</code></pre>"},{"location":"api/#llm_matrix.schema.Response","title":"<code>Response</code>","text":"<p>               Bases: <code>StrictBaseModel</code></p> <p>Response from the AI model.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class Response(StrictBaseModel):\n    \"\"\"\n    Response from the AI model.\n    \"\"\"\n    text : str = Field(..., description=\"The text of the response from the AI model\")\n    prompt: Optional[str] = Field(None, description=\"The prompt used to generate the response\")\n    system: Optional[str] = Field(None, description=\"The system prompt used to generate the response\")\n</code></pre>"},{"location":"api/#llm_matrix.schema.Template","title":"<code>Template</code>","text":"<p>               Bases: <code>StrictBaseModel</code></p> <p>Template for generating prompts to the AI model.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class Template(StrictBaseModel):\n    \"\"\"\n    Template for generating prompts to the AI model.\n    \"\"\"\n    system: Optional[FormatString] = Field(None, description=\"System prompt\")\n    prompt: Optional[FormatString] = Field(None, description=\"Prompt format string to the model\")\n    metrics: Optional[List[Metric]] = Field(None, description=\"Metrics to be evaluated\")\n</code></pre>"},{"location":"api/#llm_matrix.schema.Matrix","title":"<code>Matrix</code>","text":"<p>               Bases: <code>StrictBaseModel</code></p> <p>Specifies a combination of hyperparameters to be evaluated.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class Matrix(StrictBaseModel):\n    \"\"\"\n    Specifies a combination of hyperparameters to be evaluated.\n    \"\"\"\n    hyperparameters: Dict[Hyperparameter, List[Any]] = Field(..., description=\"Hyperparameters to be evaluated\")\n    exclude: Optional[\"Matrix\"] = Field(None, description=\"Hyperparameters to be excluded\")\n</code></pre>"},{"location":"api/#llm_matrix.schema.TestCase","title":"<code>TestCase</code>","text":"<p>               Bases: <code>StrictBaseModel</code></p> <p>Test case for the AI model.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class TestCase(StrictBaseModel):\n    \"\"\"\n    Test case for the AI model.\n    \"\"\"\n    input: str = Field(..., description=\"Input to the model\")\n    original_input: Optional[Dict[str, Any]] = Field(\n        None,\n        description=\"\"\"Original input to the model, prior to transformation into text.\n        An example is a structured data record.\n        \"\"\"\n    )\n    #output: Optional[str] = Field(None, description=\"Provided output from the model\")\n    ideal: Optional[str] = Field(None, description=\"Ideal output from the model\")\n    template: Optional[TemplateName] = Field(None, description=\"Template for the test case\")\n    tags: Optional[List[str]] = Field(None, description=\"Tags for the test case\")\n    comments: Optional[List[str]] = Field(None, description=\"Comments for the test case\")\n\n    def as_flat_dict(self, **kwargs) -&gt; Dict[str, Any]:\n        top_dict = super().as_flat_dict(**kwargs)\n        orig = self.original_input or {}\n        return {**top_dict, **orig}\n</code></pre>"},{"location":"api/#llm_matrix.schema.TestCaseResult","title":"<code>TestCaseResult</code>","text":"<p>               Bases: <code>StrictBaseModel</code></p> <p>Result of running an individual case on a model with hyperparameters.</p> Source code in <code>src/llm_matrix/schema.py</code> <pre><code>class TestCaseResult(StrictBaseModel):\n    \"\"\"\n    Result of running an individual case on a model with hyperparameters.\n    \"\"\"\n    case: TestCase = Field(..., description=\"Test case\")\n    response: Response = Field(..., description=\"Response from the model\")\n    hyperparameters: Dict[Hyperparameter, Any] = Field(..., description=\"Hyperparameters used for the test case\")\n    metrics: Optional[List[Metric]] = Field(None, description=\"Metrics to be evaluated\")\n    score: Optional[float] = Field(None, description=\"Score for the test case\", ge=0, le=1)\n    evaluation_message: Optional[str] = Field(None, description=\"Message from the evaluation\")\n\n    def as_flat_dict(self, **kwargs) -&gt; Dict[str, Any]:\n        top_dict = super().as_flat_dict(**kwargs)\n        case_dict = self.case.as_flat_dict(prefix=\"case\")\n        response_dict = self.response.as_flat_dict(prefix=\"response\")\n        hyperparameters_dict = {k: str(v) for k, v in self.hyperparameters.items()}\n        hyperparameters_str = \"_\".join(f\"{k}={v}\" for k, v in hyperparameters_dict.items())\n        return {\"hyperparameters\": hyperparameters_str, **top_dict, **case_dict, **response_dict, **hyperparameters_dict}\n</code></pre>"},{"location":"api/#metrics","title":"Metrics","text":"<p>Evaluation metrics for comparing model outputs to expected answers.</p>"},{"location":"api/#llm_matrix.metrics.MetricEvaluator","title":"<code>MetricEvaluator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for metric evaluators.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class MetricEvaluator(ABC):\n    \"\"\"Base class for metric evaluators.\"\"\"\n\n    @abstractmethod\n    def evaluate(\n        self, \n        actual_output: str, \n        expected_output: str, \n        runner: Optional[LLMRunner] = None,\n        result: Optional[TestCaseResult] = None\n    ) -&gt; float:\n        \"\"\"\n        Evaluate the result and return a score between 0 and 1.\n\n        :param actual_output: The actual output from the model\n        :param expected_output: The expected output\n        :param runner: The LLMRunner instance\n        :param result: The TestCaseResult instance, for storing evaluation messages\n        :return: A score between 0 and 1\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#llm_matrix.metrics.MetricEvaluator.evaluate","title":"<code>evaluate(actual_output, expected_output, runner=None, result=None)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the result and return a score between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>actual_output</code> <code>str</code> <p>The actual output from the model</p> required <code>expected_output</code> <code>str</code> <p>The expected output</p> required <code>runner</code> <code>Optional[LLMRunner]</code> <p>The LLMRunner instance</p> <code>None</code> <code>result</code> <code>Optional[TestCaseResult]</code> <p>The TestCaseResult instance, for storing evaluation messages</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>A score between 0 and 1</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self, \n    actual_output: str, \n    expected_output: str, \n    runner: Optional[LLMRunner] = None,\n    result: Optional[TestCaseResult] = None\n) -&gt; float:\n    \"\"\"\n    Evaluate the result and return a score between 0 and 1.\n\n    :param actual_output: The actual output from the model\n    :param expected_output: The expected output\n    :param runner: The LLMRunner instance\n    :param result: The TestCaseResult instance, for storing evaluation messages\n    :return: A score between 0 and 1\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#llm_matrix.metrics.QAWithExplanationEvaluator","title":"<code>QAWithExplanationEvaluator</code>","text":"<p>               Bases: <code>MetricEvaluator</code></p> <p>Evaluator for QA with explanation metrics.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class QAWithExplanationEvaluator(MetricEvaluator):\n    \"\"\"Evaluator for QA with explanation metrics.\"\"\"\n\n    def evaluate(\n        self, \n        actual_output: str, \n        expected_output: str, \n        runner: Optional[LLMRunner] = None,\n        result: Optional[TestCaseResult] = None\n    ) -&gt; float:\n        \"\"\"Evaluate QA with explanation result.\"\"\"\n        # First token regex\n        pattern = re.compile(r\"^(\\w+)\")\n        match = pattern.match(actual_output)\n        actual_answer = match.group(1).upper() if match else \"OTHER\"\n\n        expected_match = pattern.match(expected_output)\n        if not expected_match:\n            logger.warning(f\"Could not extract first token from expected output: {expected_output}\")\n            return 0.0\n\n        expected_answer = expected_match.group(1).upper()\n\n        if actual_answer == expected_answer:\n            score = 1.0\n        elif actual_answer == \"OTHER\" or expected_answer == \"OTHER\":\n            score = 0.5\n        else:\n            score = 0.0\n\n        return score\n</code></pre>"},{"location":"api/#llm_matrix.metrics.QAWithExplanationEvaluator.evaluate","title":"<code>evaluate(actual_output, expected_output, runner=None, result=None)</code>","text":"<p>Evaluate QA with explanation result.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def evaluate(\n    self, \n    actual_output: str, \n    expected_output: str, \n    runner: Optional[LLMRunner] = None,\n    result: Optional[TestCaseResult] = None\n) -&gt; float:\n    \"\"\"Evaluate QA with explanation result.\"\"\"\n    # First token regex\n    pattern = re.compile(r\"^(\\w+)\")\n    match = pattern.match(actual_output)\n    actual_answer = match.group(1).upper() if match else \"OTHER\"\n\n    expected_match = pattern.match(expected_output)\n    if not expected_match:\n        logger.warning(f\"Could not extract first token from expected output: {expected_output}\")\n        return 0.0\n\n    expected_answer = expected_match.group(1).upper()\n\n    if actual_answer == expected_answer:\n        score = 1.0\n    elif actual_answer == \"OTHER\" or expected_answer == \"OTHER\":\n        score = 0.5\n    else:\n        score = 0.0\n\n    return score\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator","title":"<code>LLMBasedEvaluator</code>","text":"<p>               Bases: <code>MetricEvaluator</code></p> <p>Base class for evaluators that use LLMs for scoring.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class LLMBasedEvaluator(MetricEvaluator):\n    \"\"\"Base class for evaluators that use LLMs for scoring.\"\"\"\n\n    def __init__(self, system_prompt: str, user_input_template: str):\n        self.system_prompt = system_prompt\n        self.user_input_template = user_input_template\n\n    def evaluate(\n        self, \n        actual_output: str, \n        expected_output: str, \n        runner: Optional[LLMRunner] = None,\n        result: Optional[TestCaseResult] = None\n    ) -&gt; float:\n        \"\"\"Evaluate using an LLM.\"\"\"\n        if not runner:\n            logger.error(\"Runner is required for LLM-based evaluation\")\n            return 0.0\n\n        eval_model = self._get_eval_model(runner)\n        if not eval_model:\n            logger.error(\"Could not get evaluation model\")\n            return 0.0\n\n        eval_response = self._prompt_model(eval_model, actual_output, expected_output)\n        eval_response_text = eval_response.text.strip()\n\n        if result:\n            result.evaluation_message = eval_response_text\n\n        score = self._extract_score(eval_response_text)\n        return score\n\n    def _get_eval_model(self, runner: LLMRunner):\n        \"\"\"Get the evaluation model.\"\"\"\n        if runner.config and runner.config.evaluation_model_name:\n            eval_model_name = runner.config.evaluation_model_name\n        else:\n            eval_model_name = DEFAULT_EVALUATION_MODEL_NAME\n\n        return runner.get_aimodel({\"model\": eval_model_name})\n\n    def _prompt_model(self, model, actual_output: str, expected_output: str):\n        \"\"\"Prompt the evaluation model.\"\"\"\n        user_input = self._format_user_input(actual_output, expected_output)\n\n        return model.prompt(\n            system_prompt=self.system_prompt,\n            user_input=user_input\n        )\n\n    def _format_user_input(self, actual_output: str, expected_output: str) -&gt; str:\n        \"\"\"Format the user input for the evaluation model.\"\"\"\n        return self.user_input_template.format(\n            actual_output=actual_output,\n            expected_output=expected_output\n        )\n\n    def _extract_score(self, response_text: str) -&gt; float:\n        \"\"\"Extract a score from the LLM response.\"\"\"\n        pattern = re.compile(r\"(\\d+(\\.\\d+)?)\")\n        matches = pattern.match(response_text)\n\n        if matches:\n            return float(matches.group(1))\n        else:\n            logger.error(f\"Could not parse score from {response_text}\")\n            raise ValueError(f\"Could not parse score from {response_text}\")\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator.evaluate","title":"<code>evaluate(actual_output, expected_output, runner=None, result=None)</code>","text":"<p>Evaluate using an LLM.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def evaluate(\n    self, \n    actual_output: str, \n    expected_output: str, \n    runner: Optional[LLMRunner] = None,\n    result: Optional[TestCaseResult] = None\n) -&gt; float:\n    \"\"\"Evaluate using an LLM.\"\"\"\n    if not runner:\n        logger.error(\"Runner is required for LLM-based evaluation\")\n        return 0.0\n\n    eval_model = self._get_eval_model(runner)\n    if not eval_model:\n        logger.error(\"Could not get evaluation model\")\n        return 0.0\n\n    eval_response = self._prompt_model(eval_model, actual_output, expected_output)\n    eval_response_text = eval_response.text.strip()\n\n    if result:\n        result.evaluation_message = eval_response_text\n\n    score = self._extract_score(eval_response_text)\n    return score\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator._get_eval_model","title":"<code>_get_eval_model(runner)</code>","text":"<p>Get the evaluation model.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def _get_eval_model(self, runner: LLMRunner):\n    \"\"\"Get the evaluation model.\"\"\"\n    if runner.config and runner.config.evaluation_model_name:\n        eval_model_name = runner.config.evaluation_model_name\n    else:\n        eval_model_name = DEFAULT_EVALUATION_MODEL_NAME\n\n    return runner.get_aimodel({\"model\": eval_model_name})\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator._prompt_model","title":"<code>_prompt_model(model, actual_output, expected_output)</code>","text":"<p>Prompt the evaluation model.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def _prompt_model(self, model, actual_output: str, expected_output: str):\n    \"\"\"Prompt the evaluation model.\"\"\"\n    user_input = self._format_user_input(actual_output, expected_output)\n\n    return model.prompt(\n        system_prompt=self.system_prompt,\n        user_input=user_input\n    )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator._format_user_input","title":"<code>_format_user_input(actual_output, expected_output)</code>","text":"<p>Format the user input for the evaluation model.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def _format_user_input(self, actual_output: str, expected_output: str) -&gt; str:\n    \"\"\"Format the user input for the evaluation model.\"\"\"\n    return self.user_input_template.format(\n        actual_output=actual_output,\n        expected_output=expected_output\n    )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.LLMBasedEvaluator._extract_score","title":"<code>_extract_score(response_text)</code>","text":"<p>Extract a score from the LLM response.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def _extract_score(self, response_text: str) -&gt; float:\n    \"\"\"Extract a score from the LLM response.\"\"\"\n    pattern = re.compile(r\"(\\d+(\\.\\d+)?)\")\n    matches = pattern.match(response_text)\n\n    if matches:\n        return float(matches.group(1))\n    else:\n        logger.error(f\"Could not parse score from {response_text}\")\n        raise ValueError(f\"Could not parse score from {response_text}\")\n</code></pre>"},{"location":"api/#llm_matrix.metrics.ListMembershipEvaluator","title":"<code>ListMembershipEvaluator</code>","text":"<p>               Bases: <code>LLMBasedEvaluator</code></p> <p>Evaluator for list membership metrics.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class ListMembershipEvaluator(LLMBasedEvaluator):\n    \"\"\"Evaluator for list membership metrics.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            system_prompt=(\n                \"Check if all the expected list items are present in the text. \"\n                \"Your response should be an overlap score between 0 and 1, where 1 is a perfect \"\n                \"match (all members match) and 0 is the worst possible match (no members match). \"\n                \"Your response should be the score followed by any explanatory text. \"\n                \"For example, '0.5 Only half of the items matched'. \"\n                \"Do NOT put ANY text before the score. ALWAYS start with the score. \"\n                \"Note the text you are evaluating may have additional verbiage, do not \"\n                \"penalize this. Your task is just to determine if the list is presented clearly \"\n                \"and if the items match\"\n            ),\n            user_input_template=(\n                \"The expected list is: {expected_output}. \"\n                \"The text: {actual_output}. \"\n            )\n        )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.ReviewEvaluator","title":"<code>ReviewEvaluator</code>","text":"<p>               Bases: <code>LLMBasedEvaluator</code></p> <p>Evaluator for review metrics.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class ReviewEvaluator(LLMBasedEvaluator):\n    \"\"\"Evaluator for review metrics.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            system_prompt=(\n                \"Review the output for correctness, completeness, and clarity. \"\n                \"The response should be a score between 0 (worst) and 1 (best). \"\n                \"Your response should be the score followed by any explanatory text. \"\n                \"For example, '0.3 The response has many inaccuracies'. \"\n            ),\n            user_input_template=\"The output to score is: {actual_output}. \"\n        )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.RankedListEvaluator","title":"<code>RankedListEvaluator</code>","text":"<p>               Bases: <code>LLMBasedEvaluator</code></p> <p>Evaluator for ranked list metrics.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class RankedListEvaluator(LLMBasedEvaluator):\n    \"\"\"Evaluator for ranked list metrics.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            system_prompt=(\n                \"Compare the ranked list to the expected output. \"\n                \"The response should be a score between 0 and 1. \"\n                \"If the item ranked first is equal to the expected item, score is 1. \"\n                \"If there is no overlap between the ranked list and the expected list, score is 0. \"\n                \"Otherwise score according to rank, with 0.5 for 2nd, 0.25 for 3rd, and so on.\"\n            ),\n            user_input_template=(\n                \"The expected answer is: {expected_output}. \"\n                \"The output to score is: {actual_output}. \"\n            )\n        )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.SimpleQuestionEvaluator","title":"<code>SimpleQuestionEvaluator</code>","text":"<p>               Bases: <code>LLMBasedEvaluator</code></p> <p>Evaluator for simple question metrics.</p> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>class SimpleQuestionEvaluator(LLMBasedEvaluator):\n    \"\"\"Evaluator for simple question metrics.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            system_prompt=(\n                \"Compare the answer given to the expected output. \"\n                \"The response should be a score between 0 and 1. \"\n                \"The answer should be provided first, explanations may follow \"\n                \"A precise correct answer is 1, a wrong answer is 0.\"\n                \"You can use values in between for imprecise answers\"\n            ),\n            user_input_template=(\n                \"The expected answer is: {expected_output}. \"\n                \"The output to score is: {actual_output}. \"\n            )\n        )\n</code></pre>"},{"location":"api/#llm_matrix.metrics.register_metric_evaluator","title":"<code>register_metric_evaluator(metric_name, evaluator)</code>","text":"<p>Register a custom metric evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric</p> required <code>evaluator</code> <code>MetricEvaluator</code> <p>The evaluator instance</p> required Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def register_metric_evaluator(metric_name: str, evaluator: MetricEvaluator) -&gt; None:\n    \"\"\"\n    Register a custom metric evaluator.\n\n    :param metric_name: The name of the metric\n    :param evaluator: The evaluator instance\n    \"\"\"\n    METRIC_REGISTRY[metric_name] = evaluator\n</code></pre>"},{"location":"api/#llm_matrix.metrics.evaluate_result","title":"<code>evaluate_result(result, runner=None)</code>","text":"<p>Evaluate the result of a test case.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; result = TestCaseResult(\n...    case=TestCase(input=\"What is II+IV?\", ideal=\"VI. Blah\"),\n...    response=Response(text=\"VI\"),\n...    hyperparameters={\"model\": \"gpt-4o\"},\n...    metrics=[\"qa_with_explanation\"],\n... )\n&gt;&gt;&gt; evaluate_result(result)\n&gt;&gt;&gt; result.score\n1.0\n&gt;&gt;&gt; result.response.text = \"VII\"\n&gt;&gt;&gt; evaluate_result(result)\n&gt;&gt;&gt; result.score\n0.0\n&gt;&gt;&gt; result.response.text = \"Other\"\n&gt;&gt;&gt; evaluate_result(result)\n&gt;&gt;&gt; result.score\n0.5\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TestCaseResult</code> <p>The test case result to evaluate</p> required <code>runner</code> <code>Optional[LLMRunner]</code> <p>The LLMRunner instance</p> <code>None</code> Source code in <code>src/llm_matrix/metrics.py</code> <pre><code>def evaluate_result(result: TestCaseResult, runner: Optional[LLMRunner] = None):\n    \"\"\"\n    Evaluate the result of a test case.\n\n    Example:\n\n        &gt;&gt;&gt; result = TestCaseResult(\n        ...    case=TestCase(input=\"What is II+IV?\", ideal=\"VI. Blah\"),\n        ...    response=Response(text=\"VI\"),\n        ...    hyperparameters={\"model\": \"gpt-4o\"},\n        ...    metrics=[\"qa_with_explanation\"],\n        ... )\n        &gt;&gt;&gt; evaluate_result(result)\n        &gt;&gt;&gt; result.score\n        1.0\n        &gt;&gt;&gt; result.response.text = \"VII\"\n        &gt;&gt;&gt; evaluate_result(result)\n        &gt;&gt;&gt; result.score\n        0.0\n        &gt;&gt;&gt; result.response.text = \"Other\"\n        &gt;&gt;&gt; evaluate_result(result)\n        &gt;&gt;&gt; result.score\n        0.5\n\n    :param result: The test case result to evaluate\n    :param runner: The LLMRunner instance\n    \"\"\"\n    actual_output = result.response.text\n    expected_output = result.case.ideal\n\n    scores = []\n\n    for metric_name in result.metrics or []:\n        if metric_name not in METRIC_REGISTRY:\n            raise NotImplementedError(f\"Metric {metric_name} not implemented\")\n\n        evaluator = METRIC_REGISTRY[metric_name]\n        try:\n            score = evaluator.evaluate(\n                actual_output=actual_output,\n                expected_output=expected_output,\n                runner=runner,\n                result=result\n            )\n            scores.append(score)\n        except Exception as e:\n            logger.error(f\"Error evaluating metric {metric_name}: {e}\")\n            # Optionally fall back to a default score\n            # scores.append(0.0)\n            # Or re-raise the exception\n            raise\n\n    if scores:\n        result.score = sum(scores) / len(scores)\n</code></pre>"},{"location":"api/#store","title":"Store","text":"<p>Cache for storing results and avoiding redundant API calls.</p>"},{"location":"api/#llm_matrix.store.Store","title":"<code>Store</code>  <code>dataclass</code>","text":"<p>A persistent store using DuckDB to cache test results with JSON support for Pydantic models.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; store = Store(\"test-cache.db\")\n&gt;&gt;&gt; case = TestCase(input=\"1+1\", ideal=\"2\")\n&gt;&gt;&gt; suite = Suite(name=\"test\", cases=[case], matrix={\"hyperparameters\": {}})\n&gt;&gt;&gt; response = Response(text=\"2\")\n&gt;&gt;&gt; result = TestCaseResult(case=case, response=response, hyperparameters={\"model\": \"gpt-4\"})\n&gt;&gt;&gt; store.add_result(suite, result)\n&gt;&gt;&gt; cached = store.get_result(suite, case, {\"model\": \"gpt-4\"})\n&gt;&gt;&gt; assert cached.response == response\n</code></pre> <p>To use an in-memory database, pass <code>None</code> as the <code>db_path</code>:</p> <pre><code>&gt;&gt;&gt; store = Store(None)\n</code></pre> Source code in <code>src/llm_matrix/store.py</code> <pre><code>@dataclass\nclass Store:\n    \"\"\"A persistent store using DuckDB to cache test results with JSON support for Pydantic models.\n\n    Example:\n\n        &gt;&gt;&gt; store = Store(\"test-cache.db\")\n        &gt;&gt;&gt; case = TestCase(input=\"1+1\", ideal=\"2\")\n        &gt;&gt;&gt; suite = Suite(name=\"test\", cases=[case], matrix={\"hyperparameters\": {}})\n        &gt;&gt;&gt; response = Response(text=\"2\")\n        &gt;&gt;&gt; result = TestCaseResult(case=case, response=response, hyperparameters={\"model\": \"gpt-4\"})\n        &gt;&gt;&gt; store.add_result(suite, result)\n        &gt;&gt;&gt; cached = store.get_result(suite, case, {\"model\": \"gpt-4\"})\n        &gt;&gt;&gt; assert cached.response == response\n\n    To use an in-memory database, pass `None` as the `db_path`:\n\n        &gt;&gt;&gt; store = Store(None)\n\n    \"\"\"\n    db_path: Optional[str] = None\n    _conn: Optional[duckdb.DuckDBPyConnection] = None\n\n    def __post_init__(self):\n        \"\"\"Initialize the database connection and create the table if it doesn't exist.\"\"\"\n        self._conn = duckdb.connect(str(self.db_path) if self.db_path else \":memory:\")\n        # Using JSON type for storing Pydantic models and hyperparameters\n        self._conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS results (\n                suite_name VARCHAR,\n                test_case VARCHAR,\n                ideal VARCHAR,\n                hyperparameters JSON,\n                result JSON,\n                PRIMARY KEY (suite_name, test_case, ideal, hyperparameters)\n            )\n        \"\"\")\n\n    def add_result(self, suite: Suite, result: TestCaseResult):\n        \"\"\"Add a result to the store.\"\"\"\n        self._conn.execute(\"\"\"\n            INSERT OR REPLACE INTO results \n            (suite_name, test_case, ideal, hyperparameters, result)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\n            *unique_key(suite, result.case, result.hyperparameters),\n            result.model_dump_json(exclude_unset=True),\n        ))\n        logger.debug(f\"Added result for {suite.name} {result.case} {result.hyperparameters}\")\n        self._conn.commit()\n\n    def get_result(self, suite: Suite, case: TestCase, hyperparameters: dict) -&gt; Optional[TestCaseResult]:\n        \"\"\"Get a result from the store.\"\"\"\n        result = self._conn.execute(\"\"\"\n            SELECT result\n            FROM results\n            WHERE suite_name = ?\n            AND test_case = ?\n            AND ideal = ?\n            AND hyperparameters = ?\n        \"\"\", (\n            *unique_key(suite, case, hyperparameters),\n        )).fetchone()\n\n        logger.debug(f\"Present: {result is not None} when looking up {suite.name} {case} {hyperparameters}\")\n\n        if result:\n            return TestCaseResult.model_validate_json(result[0])\n        return None\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Get the number of results in the store.\"\"\"\n        return self._conn.execute(\"SELECT COUNT(*) FROM results\").fetchone()[0]\n\n    def __del__(self):\n        \"\"\"Close the database connection when the object is destroyed.\"\"\"\n        if self._conn:\n            self._conn.close()\n</code></pre>"},{"location":"api/#llm_matrix.store.Store.size","title":"<code>size</code>  <code>property</code>","text":"<p>Get the number of results in the store.</p>"},{"location":"api/#llm_matrix.store.Store.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the database connection and create the table if it doesn't exist.</p> Source code in <code>src/llm_matrix/store.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the database connection and create the table if it doesn't exist.\"\"\"\n    self._conn = duckdb.connect(str(self.db_path) if self.db_path else \":memory:\")\n    # Using JSON type for storing Pydantic models and hyperparameters\n    self._conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS results (\n            suite_name VARCHAR,\n            test_case VARCHAR,\n            ideal VARCHAR,\n            hyperparameters JSON,\n            result JSON,\n            PRIMARY KEY (suite_name, test_case, ideal, hyperparameters)\n        )\n    \"\"\")\n</code></pre>"},{"location":"api/#llm_matrix.store.Store.add_result","title":"<code>add_result(suite, result)</code>","text":"<p>Add a result to the store.</p> Source code in <code>src/llm_matrix/store.py</code> <pre><code>def add_result(self, suite: Suite, result: TestCaseResult):\n    \"\"\"Add a result to the store.\"\"\"\n    self._conn.execute(\"\"\"\n        INSERT OR REPLACE INTO results \n        (suite_name, test_case, ideal, hyperparameters, result)\n        VALUES (?, ?, ?, ?, ?)\n    \"\"\", (\n        *unique_key(suite, result.case, result.hyperparameters),\n        result.model_dump_json(exclude_unset=True),\n    ))\n    logger.debug(f\"Added result for {suite.name} {result.case} {result.hyperparameters}\")\n    self._conn.commit()\n</code></pre>"},{"location":"api/#llm_matrix.store.Store.get_result","title":"<code>get_result(suite, case, hyperparameters)</code>","text":"<p>Get a result from the store.</p> Source code in <code>src/llm_matrix/store.py</code> <pre><code>def get_result(self, suite: Suite, case: TestCase, hyperparameters: dict) -&gt; Optional[TestCaseResult]:\n    \"\"\"Get a result from the store.\"\"\"\n    result = self._conn.execute(\"\"\"\n        SELECT result\n        FROM results\n        WHERE suite_name = ?\n        AND test_case = ?\n        AND ideal = ?\n        AND hyperparameters = ?\n    \"\"\", (\n        *unique_key(suite, case, hyperparameters),\n    )).fetchone()\n\n    logger.debug(f\"Present: {result is not None} when looking up {suite.name} {case} {hyperparameters}\")\n\n    if result:\n        return TestCaseResult.model_validate_json(result[0])\n    return None\n</code></pre>"},{"location":"api/#llm_matrix.store.Store.__del__","title":"<code>__del__()</code>","text":"<p>Close the database connection when the object is destroyed.</p> Source code in <code>src/llm_matrix/store.py</code> <pre><code>def __del__(self):\n    \"\"\"Close the database connection when the object is destroyed.\"\"\"\n    if self._conn:\n        self._conn.close()\n</code></pre>"},{"location":"api/#llm_matrix.store.unique_key","title":"<code>unique_key(suite, case, hyperparameters)</code>","text":"<p>Generate a unique key for a test result.</p> Source code in <code>src/llm_matrix/store.py</code> <pre><code>def unique_key(suite: Suite, case: TestCase, hyperparameters: dict) -&gt; tuple:\n    \"\"\"Generate a unique key for a test result.\"\"\"\n    suite_name = suite.name\n    if suite.version:\n        suite_name += f\"--{suite.version}\"\n\n    def empty(v):\n        return v is None or (isinstance(v, (str, list, dict)) and not v)\n    return suite_name, case.input, case.ideal or \"\", {k: v for k, v in hyperparameters.items() if not empty(v)}\n</code></pre>"},{"location":"api/#using-the-api","title":"Using the API","text":""},{"location":"api/#basic-example","title":"Basic Example","text":"<pre><code>from llm_matrix import LLMRunner\nfrom llm_matrix.schema import load_suite\n\n# Load a test suite from YAML\nsuite = load_suite(\"my-suite.yaml\")\n\n# Create a runner\nrunner = LLMRunner(store_path=\"results.db\")\n\n# Run the suite\nresults = runner.run(suite)\n\n# Process results\nfor result in results:\n    print(f\"Case: {result.case.input}\")\n    print(f\"Score: {result.score}\")\n    print(f\"Response: {result.response.text}\")\n</code></pre>"},{"location":"api/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from llm_matrix import LLMRunner\nfrom llm_matrix.runner import LLMRunnerConfig\n\n# Create custom configuration\nconfig = LLMRunnerConfig(\n    concurrency=5,\n    retries=3,\n    timeout=30\n)\n\n# Initialize runner with configuration\nrunner = LLMRunner(\n    store_path=\"results.db\",\n    config=config\n)\n</code></pre>"},{"location":"api/#working-with-results","title":"Working with Results","text":"<pre><code>from llm_matrix.schema import results_to_dataframe\n\n# Convert results to pandas DataFrame\ndf = results_to_dataframe(results)\n\n# Calculate statistics\nprint(df.describe())\n\n# Group by model\nmodel_performance = df.groupby(\"model\").agg({\n    \"score\": [\"mean\", \"std\", \"count\"]\n})\n</code></pre>"},{"location":"api/#creating-test-suites-programmatically","title":"Creating Test Suites Programmatically","text":"<pre><code>from llm_matrix.schema import TestSuite, TestCase, Template\n\n# Create templates\ntemplates = {\n    \"qa_template\": Template(\n        system=\"Answer the following question accurately.\",\n        prompt=\"{input}\",\n        metrics=[\"qa_simple\"]\n    )\n}\n\n# Create test cases\ncases = [\n    TestCase(\n        input=\"What is the capital of France?\",\n        ideal=\"Paris\",\n        tags=[\"geography\"]\n    ),\n    TestCase(\n        input=\"What is 2+2?\",\n        ideal=\"4\",\n        tags=[\"math\"]\n    )\n]\n\n# Create test suite\nsuite = TestSuite(\n    name=\"programmatic-suite\",\n    template=\"qa_template\",\n    templates=templates,\n    cases=cases,\n    matrix={\n        \"hyperparameters\": {\n            \"model\": [\"gpt-3.5-turbo\", \"gpt-4o\"],\n            \"temperature\": [0.0]\n        }\n    }\n)\n\n# Run the suite\nrunner = LLMRunner(store_path=\"results.db\")\nresults = runner.run(suite)\n</code></pre>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide covers how to configure LLM Matrix test suites and runners.</p>"},{"location":"configuration/#test-suite-configuration","title":"Test Suite Configuration","text":"<p>Test suites are defined in YAML files with the following structure:</p> <pre><code>name: suite-name\ntemplate: default-template-name\ntemplates:\n  template-name:\n    system: System prompt text\n    prompt: User prompt template with {variables}\n    metrics:\n      - metric_name1\n      - metric_name2\nmatrix:\n  hyperparameters:\n    model: [model1, model2]\n    temperature: [0.0, 0.7]\n    # Other parameters as needed\ncases:\n  - input: Input text for the first case\n    ideal: Expected output text\n    tags: [tag1, tag2]\n  - input: Input text for the second case\n    ideal: Expected output text\n    tags: [tag3]\n    # Case-specific parameters to override defaults\n    template: special-template \n</code></pre>"},{"location":"configuration/#core-components","title":"Core Components","text":""},{"location":"configuration/#templates","title":"Templates","text":"<p>Templates define how inputs are formatted for the LLM:</p> <pre><code>templates:\n  binary_qa:\n    system: Answer the following question with YES or NO only.\n    prompt: \"Question: {input}\"\n    metrics:\n      - binary_exact\n\n  complex_qa:\n    system: Provide a detailed answer to the following question.\n    prompt: \"Question: {input}\\nPlease explain in detail.\"\n    metrics:\n      - qa_with_explanation\n</code></pre>"},{"location":"configuration/#matrix-configuration","title":"Matrix Configuration","text":"<p>The matrix defines parameter combinations to test:</p> <pre><code>matrix:\n  hyperparameters:\n    model: [gpt-4o, gpt-3.5-turbo, claude-3-opus]\n    temperature: [0.0, 0.7]\n    max_tokens: [100, 500]\n</code></pre> <p>LLM Matrix will run each test case with every combination of these parameters.</p>"},{"location":"configuration/#test-cases","title":"Test Cases","text":"<p>Individual test cases define inputs and expected outputs:</p> <pre><code>cases:\n  - input: Is water wet?\n    ideal: \"YES\"\n    tags: [science, basic]\n\n  - input: |\n      What are the three primary colors?\n    ideal: \"The three primary colors are red, blue, and yellow.\"\n    tags: [art]\n    template: list_qa  # Override the default template\n</code></pre>"},{"location":"configuration/#runner-configuration","title":"Runner Configuration","text":"<p>You can customize the runner behavior with a separate config file:</p> <pre><code>concurrency: 5  # Number of concurrent API calls\nretries: 3      # Number of retry attempts for failed calls\ntimeout: 30     # Timeout in seconds for API calls\nplugins:\n  - citeseek    # Enable plugins\n</code></pre> <p>Pass this config to the CLI with:</p> <pre><code>llm-matrix run test-suite.yaml --runner-config runner-config.yaml\n</code></pre>"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#using-variables","title":"Using Variables","text":"<p>You can use variables in your prompts:</p> <pre><code>templates:\n  complex_prompt:\n    system: \"You are an expert in {domain}.\"\n    prompt: \"Question about {domain}: {input}\"\n</code></pre> <p>Then in your cases:</p> <pre><code>cases:\n  - input: What is DNA?\n    ideal: \"DNA is a molecule that carries genetic information...\"\n    variables:\n      domain: genetics\n</code></pre>"},{"location":"configuration/#tags-for-analysis","title":"Tags for Analysis","text":"<p>Tags help organize and filter your results:</p> <pre><code>cases:\n  - input: What is photosynthesis?\n    ideal: \"Photosynthesis is a process used by plants...\"\n    tags: [biology, plants, difficulty:easy]\n</code></pre> <p>You can later filter or group results by these tags.</p>"},{"location":"configuration/#examples","title":"Examples","text":"<p>See the examples directory for complete examples of test suite configurations.</p>"},{"location":"metrics/","title":"Evaluation Metrics","text":"<p>LLM Matrix provides various metrics to evaluate model responses against expected outputs. These metrics are specified in the template configuration.</p>"},{"location":"metrics/#available-metrics","title":"Available Metrics","text":""},{"location":"metrics/#basic-metrics","title":"Basic Metrics","text":""},{"location":"metrics/#binary_exact","title":"<code>binary_exact</code>","text":"<p>Evaluates binary (YES/NO) responses with exact matching.</p> <pre><code>templates:\n  binary_qa:\n    system: Answer the following question with YES or NO only.\n    prompt: \"Question: {input}\"\n    metrics:\n      - binary_exact\n</code></pre> <ul> <li>Score: 1.0 for exact match, 0.0 otherwise</li> <li>Best for: Simple YES/NO questions where only the answer matters, not explanation</li> </ul>"},{"location":"metrics/#qa_simple","title":"<code>qa_simple</code>","text":"<p>Simple string matching for question answering.</p> <pre><code>metrics:\n  - qa_simple\n</code></pre> <ul> <li>Score: 1.0 for exact match, partial scores for close matches</li> <li>Best for: Factual questions with specific answers</li> </ul>"},{"location":"metrics/#explanation-metrics","title":"Explanation Metrics","text":""},{"location":"metrics/#qa_with_explanation","title":"<code>qa_with_explanation</code>","text":"<p>Evaluates both the answer and explanation.</p> <pre><code>metrics:\n  - qa_with_explanation\n</code></pre> <ul> <li>Score: Combined score for answer correctness and explanation quality</li> <li>Best for: Questions requiring both correct answers and explanations</li> </ul>"},{"location":"metrics/#binary_with_explanation","title":"<code>binary_with_explanation</code>","text":"<p>For YES/NO questions with explanations.</p> <pre><code>metrics:\n  - binary_with_explanation\n</code></pre> <ul> <li>Score: 1.0 for correct binary answer with good explanation, lower for partial matches</li> <li>Best for: Binary questions where explanation is important</li> </ul>"},{"location":"metrics/#list-metrics","title":"List Metrics","text":""},{"location":"metrics/#list_comparison","title":"<code>list_comparison</code>","text":"<p>Compares lists of items.</p> <pre><code>metrics:\n  - list_comparison\n</code></pre> <ul> <li>Score: Based on overlap between expected and actual lists</li> <li>Best for: Enumeration tasks (e.g., \"List the planets in the solar system\")</li> </ul>"},{"location":"metrics/#creating-custom-metrics","title":"Creating Custom Metrics","text":"<p>You can create custom metrics by implementing the <code>Metric</code> class:</p> <pre><code>from llm_matrix.metrics import Metric\nfrom llm_matrix.schema import EvalResult, TestCase, LLMResponse\n\nclass MyCustomMetric(Metric):\n    def evaluate(self, case: TestCase, response: LLMResponse) -&gt; EvalResult:\n        # Implement your evaluation logic\n        score = calculate_score(case.ideal, response.text)\n\n        return EvalResult(\n            score=score,\n            explanation=\"Reason for this score\"\n        )\n</code></pre> <p>Register your custom metric:</p> <pre><code>from llm_matrix.metrics import register_metric\n\nregister_metric(\"my_custom_metric\", MyCustomMetric())\n</code></pre> <p>Then use it in your templates:</p> <pre><code>templates:\n  my_template:\n    system: Custom prompt\n    prompt: \"{input}\"\n    metrics:\n      - my_custom_metric\n</code></pre>"},{"location":"metrics/#best-practices","title":"Best Practices","text":"<ul> <li>Choose metrics appropriate for your task type</li> <li>Consider using multiple metrics for complex tasks</li> <li>For binary tasks, prefer specialized binary metrics over general ones</li> <li>When using list-based metrics, ensure your ideal answer is formatted as expected</li> <li>For most accurate scoring, define clear evaluation criteria in your prompts</li> </ul>"},{"location":"metrics/#metrics-api-reference","title":"Metrics API Reference","text":"<p>For detailed API documentation of all metrics, see the Metrics API Reference.</p>"},{"location":"tutorial/","title":"Getting Started with LLM Matrix","text":"<p>This tutorial walks you through the basic usage of LLM Matrix, from installation to running your first evaluation.</p>"},{"location":"tutorial/#installation","title":"Installation","text":"<p>Install LLM Matrix using pip:</p> <pre><code>pip install llm-matrix\n</code></pre>"},{"location":"tutorial/#creating-your-first-test-suite","title":"Creating Your First Test Suite","text":"<p>Create a file named <code>first-test.yaml</code> with the following content:</p> <pre><code>name: my-first-test\ntemplate: simple_qa\ntemplates:\n  simple_qa:\n    system: Answer the following question accurately and concisely.\n    prompt: \"{input}\"\n    metrics:\n      - qa_simple\nmatrix:\n  hyperparameters:\n    model: [gpt-3.5-turbo]\n    temperature: [0.0]\ncases:\n  - input: What is the capital of France?\n    ideal: Paris\n    tags: [geography]\n  - input: What is 2+2?\n    ideal: \"4\"\n    tags: [math]\n</code></pre>"},{"location":"tutorial/#understanding-the-yaml-structure","title":"Understanding the YAML Structure","text":"<ul> <li><code>name</code>: Identifier for your test suite</li> <li><code>template</code>: Default template to use</li> <li><code>templates</code>: Define different prompt templates</li> <li><code>system</code>: System instructions to the model</li> <li><code>prompt</code>: Template for user prompt (with variables in curly braces)</li> <li><code>metrics</code>: Evaluation metrics to apply</li> <li><code>matrix.hyperparameters</code>: Parameters to vary in the test</li> <li><code>cases</code>: Individual test cases</li> <li><code>input</code>: Query to send to the model</li> <li><code>ideal</code>: Expected response (used for evaluation)</li> <li><code>tags</code>: Optional categorization</li> </ul>"},{"location":"tutorial/#running-the-test-suite","title":"Running the Test Suite","text":"<p>Execute your test suite with:</p> <pre><code>llm-matrix run first-test.yaml\n</code></pre> <p>This will: 1. Create a database file to cache results 2. Run each test case against the specified model(s) 3. Generate evaluation reports</p>"},{"location":"tutorial/#viewing-results","title":"Viewing Results","text":"<p>After running the test suite, you'll find a <code>first-test-output</code> directory with:</p> <ul> <li><code>results.csv</code>: Raw results for all runs</li> <li><code>summary.csv</code>: Statistical summary of model performance</li> <li><code>by_model.csv</code>: Performance breakdown by model</li> <li><code>grouped_by_input.tsv</code>: Detailed breakdown by test case</li> </ul>"},{"location":"tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Configuring Test Suites: Learn about advanced configuration options</li> <li>Understanding Metrics: Explore different evaluation metrics</li> <li>CLI Reference: Discover all command-line options</li> </ul>"}]}